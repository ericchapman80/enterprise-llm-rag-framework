# RAG-LLM Framework Environment Variables
# Copy this file to .env and fill in your values

# GitHub Integration
GITHUB_TOKEN=your_github_token_here

# Slack Bot Integration
SLACK_BOT_TOKEN=xoxb-your-bot-token-here
SLACK_APP_TOKEN=xapp-your-app-token-here
SLACK_SIGNING_SECRET=your-signing-secret-here

# LLM Configuration
# Uncomment to override default values in Helm chart
# OLLAMA_MODEL_NAME=llama2
# OLLAMA_TEMPERATURE=0.7
# OLLAMA_TOP_P=0.9
# OLLAMA_MAX_TOKENS=2048

# Model Storage Configuration
# OLLAMA_STORAGE_TYPE=local  # Options: local, cloud
# OLLAMA_CLOUD_PROVIDER=s3   # Options: s3, gcs, azure, nfs
# OLLAMA_CACHE_DIR=/tmp/ollama_cache

# S3 Configuration (when OLLAMA_CLOUD_PROVIDER=s3)
# S3_BUCKET=your-models-bucket
# S3_REGION=us-west-2
# S3_PREFIX=models/
# S3_ACCESS_KEY=your-access-key
# S3_SECRET_KEY=your-secret-key

# GCS Configuration (when OLLAMA_CLOUD_PROVIDER=gcs)
# GCS_BUCKET=your-models-bucket
# GCS_PREFIX=models/
# GCS_CREDENTIALS_FILE=/path/to/credentials.json

# Azure Configuration (when OLLAMA_CLOUD_PROVIDER=azure)
# AZURE_CONTAINER=your-container
# AZURE_STORAGE_ACCOUNT=your-account
# AZURE_STORAGE_KEY=your-key

# NFS Configuration (when OLLAMA_CLOUD_PROVIDER=nfs)
# NFS_PATH=/mnt/nfs/models

# Vector Database Configuration
# Uncomment to override default values in Helm chart
# EMBEDDINGS_MODEL_NAME=all-MiniLM-L6-v2

# Multi-threading Configuration
RAG_INGESTION_THREADS=4  # Use 4 threads for ingestion
